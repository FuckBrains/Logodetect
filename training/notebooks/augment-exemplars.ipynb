{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Training Dataset With Image Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and process images\n",
    "\n",
    "Remove transparency:\n",
    "```\n",
    "mogrify -background white -flatten ./*.png\n",
    "```\n",
    "\n",
    "Resize as necessary:\n",
    "```\n",
    "find . -name \"*.png\" -print0 | xargs -0 mogrify -resize 20%\n",
    "```\n",
    "or\n",
    "```\n",
    "mogrify -resize 3720x5260! ./*.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image as p_image\n",
    "import imgaug.augmenters as ia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment images:\n",
    "PATH = os.path.join(os.environ['HKT'], 'Logos-Recognition-Training', 'data')\n",
    "FORE_FILES = os.path.join(PATH, 'exemplars_100x100', '*.jpg')\n",
    "BACK_FILES = ''\n",
    "AUGMENT_PATH = os.path.join(PATH, 'exemplars_100x100_aug')\n",
    "PARAMS = {\n",
    "    'Multiply': [0.5, 1.5],\n",
    "    'GaussianBlur': [0.4],\n",
    "    'AdditiveGaussianNoise': [0.2*255],\n",
    "    'AffineShear': [-25, 25],\n",
    "    'AffineRotate': [-25, 25],\n",
    "}\n",
    "COLOR_SPACE = 'RGB'\n",
    "MAX_INST = np.Inf\n",
    "PROP = 1\n",
    "\n",
    "# Compose images:\n",
    "COMPOSE_PATH = ''\n",
    "CSV_COLUMNS_TF = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "CSV_COLUMNS_KERAS = ['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'class']\n",
    "MARGIN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    "    image = p_image.open(filename).convert(COLOR_SPACE)\n",
    "    return np.expand_dims(image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(array, filename):\n",
    "    image = p_image.fromarray(array.squeeze()).convert(COLOR_SPACE)\n",
    "    image.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencer(mu, gabl, adga, afsh, afro):\n",
    "    sequence = ia.Sequential([\n",
    "        ia.Multiply(mul=mu),\n",
    "        ia.GaussianBlur(sigma=gabl),\n",
    "        ia.AdditiveGaussianNoise(scale=adga),\n",
    "        ia.Affine(rotate=afro, shear=afsh),\n",
    "    ])\n",
    "    extension = '_mult' + str(  mu) + '_gabl' + str(gabl) +\\\n",
    "                '_adga' + str(adga) + '_afsh' + str(afsh) +\\\n",
    "                '_afro' + str(afro)\n",
    "    return sequence, extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_and_save(mu, gabl, adga, afsh, afro,\n",
    "                     proportion, file, counter, max_inst,\n",
    "                     save_path):\n",
    "    \n",
    "    # 3^12 = 531 441, therefore only make a transformation with p = proportion:\n",
    "    if (counter < max_inst) and (np.random.uniform() < proportion):\n",
    "\n",
    "        # Process image:\n",
    "        seq, ext = sequencer(mu, gabl, adga, afsh, afro)\n",
    "        image = load_image(file)\n",
    "        aug_image = seq.augment_images(image)\n",
    "\n",
    "        # Store image:\n",
    "        counter += 1\n",
    "        ending = '_' + str(counter).zfill(3) + ext + '_aug'\n",
    "        basename = os.path.basename(file)\n",
    "        name, extension = os.path.splitext(basename)\n",
    "        new_name = name + ending + extension\n",
    "        new_file = os.path.join(save_path, new_name)\n",
    "        save_image(aug_image, new_file)\n",
    "        return counter\n",
    "    \n",
    "    else:\n",
    "        return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352 source images to process.\n",
      "Approx. 8 variations per file.\n",
      "Limit set to max. inf variations per file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [00:09<00:00, 35.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Source images to crate augmentations:\n",
    "augment_files = sorted(glob.glob(FORE_FILES))\n",
    "n_variations = 1\n",
    "for values in PARAMS.values():\n",
    "    n_variations *= len(values)\n",
    "\n",
    "print(len(augment_files), 'source images to process.', flush=True)\n",
    "print('Approx.', int(n_variations * PROP), 'variations per file.', flush=True)\n",
    "print('Limit set to max.', MAX_INST, 'variations per file.', flush=True)\n",
    "\n",
    "for file in tqdm(augment_files):\n",
    "    counter = 0\n",
    "\n",
    "    # For each combination:\n",
    "    for mu in PARAMS['Multiply']:\n",
    "        for gabl in PARAMS['GaussianBlur']:\n",
    "            for adga in PARAMS['AdditiveGaussianNoise']:\n",
    "                for afsh in PARAMS['AffineShear']:\n",
    "                    for afro in PARAMS['AffineRotate']:\n",
    "\n",
    "                        # Process image:\n",
    "                        counter = augment_and_save(  mu, gabl, adga, afsh, afro,\n",
    "                                                   PROP, file, counter, MAX_INST,\n",
    "                                                   AUGMENT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_array_and_label(image_name, back_width, back_heigth, logo_class,\n",
    "                            y_start, x_start, logo_width, logo_heigth,\n",
    "                            images_list, back_arr, logo_arr):\n",
    "    \n",
    "    # Compose label:\n",
    "    x_end = x_start + logo_width\n",
    "    y_end = y_start + logo_heigth\n",
    "    image_data = (image_name,   # filename\n",
    "                  back_width,   # width\n",
    "                  back_heigth,  # height\n",
    "                  logo_class,   # class\n",
    "                  x_start,      # xmin\n",
    "                  y_start,      # ymin\n",
    "                  x_end,        # xmax\n",
    "                  y_end)        # ymax\n",
    "    images_list.append(image_data)\n",
    "    \n",
    "    # Compose array:\n",
    "    back_arr[y_start:y_end, x_start:x_end] = logo_arr\n",
    "    \n",
    "    return images_list, back_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available augmented backgounds: 100\n",
      "Available augmented logos:      60\n"
     ]
    }
   ],
   "source": [
    "logo_files = sorted(glob.glob(FORE_FILES))  # Augmented foregrounds to combine.\n",
    "back_files = sorted(glob.glob(BACK_FILES))  # Augmented backgrounds to combine.\n",
    "print('Available augmented backgounds:', len(back_files))\n",
    "print('Available augmented logos:     ', len(logo_files))\n",
    "\n",
    "images_list = []\n",
    "for logo_file in logo_files:\n",
    "    # Define array parameters:\n",
    "    logo_arr = np.array(p_image.open(logo_file).convert(COLOR_SPACE))\n",
    "    logo_heigth = np.shape(logo_arr)[0]  # Height\n",
    "    logo_width = np.shape(logo_arr)[1]  # Width\n",
    "    \n",
    "    # Define label parameters:\n",
    "    logo_name =      logo_file.split('/')[-1]\n",
    "    logo_class =     logo_name.split('_')[0]\n",
    "    logo_class_num = logo_name.split('_')[1].zfill(3)\n",
    "    logo_num =       logo_name.split('_')[2].zfill(3)\n",
    "    logo_new_name = '{}_{}_{}'.format(logo_class, logo_class_num, logo_num)\n",
    "    \n",
    "    for back_file in back_files:\n",
    "        # Define array parameters:\n",
    "        back_arr = np.array(p_image.open(back_file).convert(COLOR_SPACE))\n",
    "        back_heigth = np.shape(back_arr)[0]  # Height\n",
    "        back_width = np.shape(back_arr)[1]  # Width\n",
    "        \n",
    "        # Define label parameters:\n",
    "        back_name =      back_file.split('/')[-1]\n",
    "        back_class =     back_name.split('_')[0]\n",
    "        back_class_num = back_name.split('_')[1].zfill(3)\n",
    "        back_num =       back_name.split('_')[2].zfill(3)\n",
    "        back_new_name = '{}_{}_{}'.format(back_class, back_class_num, back_num)\n",
    "        \n",
    "        # NOTE: PNG creates files 3x larger than JPG.\n",
    "        image_name = '{}_{}.jpg'.format(logo_new_name, back_new_name)\n",
    "        \n",
    "        min_u = int(0 + MARGIN)\n",
    "        max_u = int(back_heigth / 2 - logo_heigth - MARGIN)\n",
    "        \n",
    "        min_l = int(0 + MARGIN)\n",
    "        max_l = int(back_width / 2 - logo_width - MARGIN)\n",
    "        \n",
    "        min_d = int(back_heigth / 2 + MARGIN)\n",
    "        max_d = int(back_heigth - logo_heigth - MARGIN)\n",
    "        \n",
    "        min_r = int(back_width / 2 + MARGIN)\n",
    "        max_r = int(back_width - logo_width - MARGIN)\n",
    "        \n",
    "        # Verify thet the logo is not too big:\n",
    "        assert (max_u - min_u) > 0\n",
    "        assert (max_d - min_d) > 0\n",
    "        assert (max_l - min_l) > 0\n",
    "        assert (max_r - min_r) > 0\n",
    "        \n",
    "        # UP and LEFT:\n",
    "        u_start = np.random.randint(min_u, max_u)\n",
    "        l_start = np.random.randint(min_l, max_l)\n",
    "        images_list, back_arr = compose_array_and_label(image_name, back_width, back_heigth, logo_class,\n",
    "                                                        u_start, l_start, logo_width, logo_heigth,\n",
    "                                                        images_list, back_arr, logo_arr)\n",
    "        # UP and RIGHT:\n",
    "        u_start = np.random.randint(min_u, max_u)\n",
    "        r_start = np.random.randint(min_r, max_r)\n",
    "        images_list, back_arr = compose_array_and_label(image_name, back_width, back_heigth, logo_class,\n",
    "                                                        u_start, r_start, logo_width, logo_heigth,\n",
    "                                                        images_list, back_arr, logo_arr)\n",
    "        # DOWN and LEFT:\n",
    "        d_start = np.random.randint(min_d, max_d)\n",
    "        l_start = np.random.randint(min_l, max_l)\n",
    "        images_list, back_arr = compose_array_and_label(image_name, back_width, back_heigth, logo_class,\n",
    "                                                        d_start, l_start, logo_width, logo_heigth,\n",
    "                                                        images_list, back_arr, logo_arr)\n",
    "        # DOWN and RIGHT:\n",
    "        d_start = np.random.randint(min_d, max_d)\n",
    "        r_start = np.random.randint(min_r, max_r)\n",
    "        images_list, back_arr = compose_array_and_label(image_name, back_width, back_heigth, logo_class,\n",
    "                                                        d_start, r_start, logo_width, logo_heigth,\n",
    "                                                        images_list, back_arr, logo_arr)\n",
    "        # Save final image:\n",
    "        save_image(back_arr, COMPOSE_PATH + image_name)\n",
    "        \n",
    "    # Compose TF labels CSV:\n",
    "    image_df = pd.DataFrame(images_list, columns=CSV_COLUMNS_TF)\n",
    "    image_df.to_csv(COMPOSE_PATH + '/train_labels_tf.csv', index=False, mode='w')\n",
    "    \n",
    "    # Compose KERAS labels CSV:\n",
    "    image_df.drop(columns=['width', 'height'])\n",
    "    image_df.to_csv(COMPOSE_PATH + '/train_labels_keras.csv', index=False, mode='w',\n",
    "                    columns=CSV_COLUMNS_KERAS, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
